{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analyzer using Convolutional Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnukanduri/Tweet-Sentiment-Analyzer-using-CNN/blob/master/Sentiment_analyzer_using_Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9pcOKQk7Bt8",
        "colab_type": "text"
      },
      "source": [
        "Let's delve into how computers deal with human language. \n",
        "\n",
        "Natural Language Processing (NLP) is the part of AI (Artificial Intelligence) that deals with human language. We want computers to be able to grasp the meaning between words which is not evident since computers only deal with sequences of zeroes and ones. \n",
        "\n",
        "Sentiment analysis is one of the essential tasks done in NLP. We will build a sentiment analyzer using CNN (Convolutional Neural Network). It will take as inputs tweets and outputs whether it conveys a positive or negative sentiment. \n",
        "\n",
        "CNN was first used to process images. For example, at the very beginning, it was used to recognize digits that were handwritten and also to recognize the contents of an image. Researchers soon realized that CNN could also be a powerful tool for language processing. Classification tasks are easily addressed with CNNs. \n",
        "\n",
        "First, we will go into the origins of CNN (how it works for images, how does it work and what are its components). We will understand how text is linked to images and CNNs for text. CNN takes as input an image and outputs a label (image class). \n",
        "\n",
        "##Steps in CNN:\n",
        "\n",
        "A crucial one and the first step is convolution. Convolution creates a lot of feature detectors that scan the entire image and gives us a list of feature maps whether or not and where a specific feature appears in the image.\n",
        "\n",
        "Next operation is Max Pooling, where we just apply the maximum function to all feature maps so that we can make them smaller and improve the performance of the model.\n",
        "\n",
        "Then comes the straightforward flattening phase where we make a massive vector out of all the maps which are matrices.\n",
        "\n",
        "We end with a feed-forward neural network which learns from the feature extraction phase. \n",
        "\n",
        "Convolution consists of feature detectors applying over the entire image. We go through patches in the image that have the same size as that of our feature detector and perform an element-wise multiplication and sum all the products. This result might not make any sense for us humans, but when this is connected with a feed-forward neural network, CNN can achieve great results with it. \n",
        "\n",
        "A CNN consists of a lot of feature detectors, and each detector will provide a feature map. The feature detector contains randomly initialized values, and those are the variables learned throughout the process of training. \n",
        "\n",
        "After comparing the results after one cycle of prediction with the actual correct labels, the model will tune those numbers to learn to detect more useful and accurate features. \n",
        "\n",
        "Next, in the max-pooling operation, we don't take the global maximum but maximum of kernel size. Max-pooling reduces size and computation. The idea is we need not know everything about whether the feature appeared in the image or not. The most important thing is to see whether it seems or not. But we still need to keep some locality in this process. \n",
        "\n",
        "We still want to keep the relations between the positions of the features. There exists a tradeoff between getting rid of the information we don't need and also making the map smaller so that we can improve the process and reduce the costs. We apply this to several feature maps, and we get the same number of pooled maps. \n",
        "\n",
        "Flattening converts into a vector of 1 dimension to be as input to the feed-forward neural network. We transform each pooled map into a vector. It keeps the locality information (position). \n",
        "\n",
        "We can add as many hidden layers required in the feed-forward neural network. Higher the output value, higher the probability that input belongs to that class. \n",
        "\n",
        "Similar to the way we looked for features in the images, we can look for features in the text. For that, a sentence should be converted into a matrix as images were matrices. \n",
        "\n",
        "The easiest way of representing a sentence as a vector is also the most inefficient one. It's called one-hot encoding. Vocabulary consists of all the unique words in our corpus (text dataset). Each vector is of vocabulary size. All the entries in each vector will be all zeroes except for one entry which will have a value of one. It doesn't convey any relation between words. \n",
        "\n",
        "Reducing the size of vectors enforces less liberty, and that forces our model to create relations. Instead of being binary, each value in the vector dimension will have a value between 0 and 1. Words with similar meaning will be closer in the embedding space. \n",
        "\n",
        "Image matrices have a meaning when moved from left to right and top to bottom. But word matrices only have a purpose when moved from top to bottom. Because the right to left movement indicates the dimension of that vector, it doesn't make sense to perform 2D convolution. The width of the filter or kernel would be the same as that of the embedding dimension. 1D convolution implemented as feature extraction along one dimension is needed. \n",
        "\n",
        "Global maximum of the convolution operation is taken since the position of a feature in a sentence is not as important as that in an image. We care if that feature is present or not. A filter of different sizes is used for convolution as opposed to that when working with images. \n",
        "\n",
        "Flattening phase is not required as the output of the convolution is a vector and not a matrix. \n",
        "\n",
        "We are done with the theory and let's jump into the implementation. The implementation will be done in Google Colab. It offers free GPU and TPU instances which speeds up the code execution. You will also learn how to use Google Colab in this post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6zhycx-LJa",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NON9giQ1_eZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re #regex for string cleaning\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import drive\n",
        "#The best way to use files in Google colab is via Google Drive. So, we import drive module to connect it with Google colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un7jTbQNedR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We are asking for the Tensorflow version of 2.x (it can be 2.1, 2.0.2 or any such ones but it should start with 2)\n",
        "#If it doesn't have that it gives any version it has\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers #Used to create layers in our deep learning model\n",
        "import tensorflow_datasets as tfds #Tensorflow datasets are ready-to-use datasets with Tensorflow or other Python ML frameworks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8EilQG-cwa",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52CTV4_q-hpX",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAMosfNZ--bk",
        "colab_type": "text"
      },
      "source": [
        "Obviously, the first step is to load our files. As already told, drive is the best way of accessing files in Google Colab. The mount method of drive gives us the ability to connect one's Drive to Colab.\n",
        "\n",
        "'/content/drive' is the path at which Drive is located"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "solg5Rzx6-oY",
        "colab_type": "code",
        "outputId": "32fb758d-b174-40b0-a77d-9b08d6ee37e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#When you run this, it asks you to go to a URL for authentication. Once you land on the page, copy the code available and paste it in the textbox titled\n",
        "#'Enter your authorization code' and hit enter.\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp9srJIb_K9W",
        "colab_type": "text"
      },
      "source": [
        "###cols is the list of column names of our dataset.\n",
        "\n",
        "`sentiment` : indicates if the sentiment is positive or negative. 0 denotes negative, 1 denotes positive\n",
        "\n",
        "`id` : ID of the tweet\n",
        "\n",
        "`date` : date on which the tweet was sent\n",
        "\n",
        "`query` : this column is not very useful. All the values for this column are 'NO_QUERY' which means none of the tweets have any query.\n",
        "\n",
        "`user` : the user who tweeted (Twitter handle)\n",
        "\n",
        "`text` : the tweet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Qkr_1Zfgi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDNqaBjK_vsR",
        "colab_type": "text"
      },
      "source": [
        "####**We will be using Pandas to work with our data. Pandas stores the data in a dataframe which is a neat tabular representation of the data.**\n",
        "Our dataset is a CSV file (Comma separated value) which means all our values in a line (a row if you think of the data as being in a table) are separated by commas.\n",
        "\n",
        "We use the read_csv method to read in the CSV file.\n",
        "\n",
        "`First argument`: The Path to the file\n",
        "\n",
        "`Second argument`: We don't have column names in the first row. Which means there is no header and we don't want Pandas to use the first row as the header. So, header = None\n",
        "\n",
        "`Third argument`: names = column names which are stored in the cols list. So, we set names = cols\n",
        "\n",
        "`Fourth argument`: an optional argument, specifies which engine to use to parse the data, we select Python\n",
        "\n",
        "`Fifth argument`: encoding to use for the file. You can think of it as the language standard. We specify latin1, just in case we have tweets in other languages like Chinese, Arabic or Hebrew or words with accents unlike in English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weXgs3ul_nz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We store our train.csv file in a variable called train_data to use it later for other steps.\n",
        "train_data = pd.read_csv(\n",
        "    \"/content/drive/My Drive/NLP/data.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    skiprows = 1,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXtSGYQpZxH",
        "colab_type": "code",
        "outputId": "b77a739f-d4d5-46ff-d0d1-f362b71d1598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Shape returns the number of rows and columns in our dataset. There are 30000 rows and 6 columns\n",
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMbB2weunMKZ",
        "colab_type": "code",
        "outputId": "7412f86b-784b-48ea-fc18-42be84d86f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#Seeing first five rows of our dataset\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1553795194</td>\n",
              "      <td>Sat Apr 18 15:13:59 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>t_win</td>\n",
              "      <td>It's been the longest day ever! I still haven'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2179002334</td>\n",
              "      <td>Mon Jun 15 08:30:28 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>badsotheynv</td>\n",
              "      <td>I feel uber bad little ol lady is sick wanted ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1936039755</td>\n",
              "      <td>Wed May 27 07:20:42 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mubi_just_do_it</td>\n",
              "      <td>goose just died...saddest scene i've seen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2185132296</td>\n",
              "      <td>Mon Jun 15 16:56:05 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>walkthistown</td>\n",
              "      <td>@alexamarzi I KNOWW dont move</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2180496762</td>\n",
              "      <td>Mon Jun 15 10:33:02 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>clare666</td>\n",
              "      <td>@Piewacket1 awwww pie... the 'once in a lifeti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  It's been the longest day ever! I still haven'...\n",
              "1          0  ...  I feel uber bad little ol lady is sick wanted ...\n",
              "2          0  ...      goose just died...saddest scene i've seen... \n",
              "3          0  ...                     @alexamarzi I KNOWW dont move \n",
              "4          0  ...  @Piewacket1 awwww pie... the 'once in a lifeti...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-6BBPb3-OfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's have our mastercopy as it is, if we want to return to this file or see how the original train.csv looked\n",
        "#So, we create a variable called data and assign it train_data to make a copy of it.\n",
        "data = train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CsZKJx1-2Ep",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLWayJ-5-7nN",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcIP66ulKfAH",
        "colab_type": "text"
      },
      "source": [
        "The columns important for our text analysis is only the tweet and it's sentiment. We can remove all other columns. This has 2 advantages:\n",
        "1. Smaller dataframes leads to higher performance\n",
        "2. We can focus on what is required and need not worry or get disturbed by unimportant columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qa1v91RSkgz1",
        "colab": {}
      },
      "source": [
        "#Pandas gives a method called drop to drop rows or columns from the dataframe.\n",
        "#We need to specify which columns to drop (remove), axis : if 0: removes the row, if 1: removes that column\n",
        "#Once this cell is executed, Pandas modifies our dataframe only for this cell but not permanently. To permanently modify the dataframe, there are 2 methods:\n",
        "#1. Assigning this statement to data (the variable containing our data). In this we overwrite the old data\n",
        "#2. Using the inplace argument, setting it to True tells it to modify it in the original dataframe.\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI9mX24GMDNL",
        "colab_type": "text"
      },
      "source": [
        "####Tweets may contain punctuations, white spaces, URL links. They may be in case, upper or lower. To use a piece of code repeatedly, a function can be used. Since we have a lot of tweets and every tweet needs to be cleaned. This calls for the perfect situation to use a function. To bring all tweets to a standard, let's define a function to clean tweets. The name of the function is clean_tweet. It takes in input as a tweet.\n",
        "\n",
        "**BeautifulSoup** is a Python library used to parse HTML. HTML stands for Hyper Text Markup Language. This is the language behind the Internet. Every webpage and website uses HTML to structure the layout and define components. HTML is dirty, apart from the text we need it contains a lot of tags, formatting, structuring components. This is where BeautifulSoup comes in, it parses HTML using any of the parsers like lxml, html5lib, html.parser. We also have the luxury to choose the parser we want. lxml is the best parser among all.\n",
        "\n",
        "`Line 1`: Creates a BeautifulSoup object with lxml parser. The first argument is the data we want to parse which is the tweet in our case. get_text() returns only the text.\n",
        "\n",
        "For text cleaning, regex is the best library. It allows us to specify patterns and search for strings that follow a pattern. We have imported it using the line 'import re' in the first cell.\n",
        "\n",
        "`Line 2`: Many tweets mention other users using '@'. This needs to be removed. Removing can also be thought of as replacing that text with a whitespace. Regex provides a method called sub which replaces the value in 1st argument by 2nd arg in the text specified by 3rd arg.\n",
        "\n",
        "`1st arg`: r indicates the start of regex. Follows the pattern specified within the quotes. The pattern is '@', 'any upper or lower case characters or numerals between 0 and 9' (A to Z, a to z, 0 to 9 is indicated as [A-Za-z0-9], [] specifies a class or group of characters), + indicates any number of them\n",
        "\n",
        "`2nd arg`: a space in quotes => Whitespace\n",
        "\n",
        "`3rd arg`: tweet\n",
        "\n",
        "The next step is to remove any links in the tweet. Links start with https or http follwed by :// and any number of characters or numerals. In regex, this is specified as r: start of regex, pattern within quotes. \n",
        "\n",
        "`1st arg`: Since links can either start with http or https, we need to make s an optional character. '?' makes the preceding character optional. So, '?' should be used after https. Then '://', [A-Za-z0-9] to indicate the content of the URL. '+' indicates any number of such characters. \n",
        "\n",
        "Next, keeping only letters and common punctuations used in text like '.', '!', '?' and '' (apostrophe)'. The same explanation as given for above 2 steps works here too.\n",
        "\n",
        "Since we have replaced all of the unnecessary stuff by whitespaces, a lot of whitespaces would exist. As you expect, last step is to remove all whitespaces. To indicate any number of whitespaces, we use ' +' (whitespace followed by +).\n",
        "\n",
        "After the manipulation, return tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU-7WW0m9O5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-jKPI2MzuXT",
        "colab_type": "text"
      },
      "source": [
        "Now, we have to call this function on all our tweets. A for loop can be used, but a much more compact way is list comprehension (only takes 1 line). \n",
        "\n",
        "We say call the function clean_tweet on each tweet in the text column of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-mIZNdAUjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlOwH7T90J14",
        "colab_type": "text"
      },
      "source": [
        "We can see the possible values a column and their respective count using value_counts(). Let's apply this on the sentiment column. We see that 4 is used instead of 1 to denote positive sentiments. So, all the occurrences of 4 has to be replaced by 1. Using the values attribute on any column returns an array with all column values. \n",
        "\n",
        "A very powerful concept used in Pandas is boolean masking. An example of this is used below. 'data_labels == 4' returns either True if a value is 4 or False if a value is not 4, which means 0 (since 0 and 4 are the only possible values). We can use this True, False list as an index to decide which values need to be replaced by 1. Wherever there is True, it replaces that value by 1. The values with zero remain as is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yGBgjAP0ro4",
        "colab_type": "code",
        "outputId": "ec369ff2-eb94-4920-ae3f-1986a5a913ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "data['sentiment'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    15000\n",
              "0    15000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqtCJZkhAb9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXNQASg5m63G",
        "colab_type": "code",
        "outputId": "7974524e-bcb8-44dc-8802-a3db8a84f6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKZ5fUh_Kxz",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v32XPymL15PR",
        "colab_type": "text"
      },
      "source": [
        "Now that we are done with preprocessing steps, let's get into the meat of this project. Tokenization is the first part in any NLP task. Tokenizer is separating the sentence into different components. A word tokenizer splits by every word. A sentence tokenizer splits by a sentence. It is also what converts a list of characters into numbers. If we manually make up a list of numbers for every word or sentence, it will be tedious and also will not be able to convert any given word into a list of numbers.  \n",
        "\n",
        "Luckily for us, tensorflow datasets which we had imported before. We just give it the corpus (the tweets) and the Vocabulary size (number of unique words). It can even be less than the number of words we wish to have. In that way, the encoder can compose a word with another if it can't find an unique representation for a word. It can be useful and powerful sometimes for words that appear very less number of times in the corpus. It will build an encoder which is an object that can transform any string to a list of numbers. We will have our vocab size as 64000 words. It takes quite sometime to create the tokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvmIKnAnAJRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    data_clean, target_vocab_size=2**11\n",
        ")\n",
        "\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysb2uib8n6b3",
        "colab_type": "text"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irnd_MqAZFs0",
        "colab_type": "text"
      },
      "source": [
        "We don't train with a single exmaple at a time. We train in batches. But in order to do that, we need all tweets to have the same length. A simple way is to add zeros at the end of each sentence so that they all have the same length. An important thing is that zero is not a number that is used by our tokenizer. Zero doesn't have any meaning and doesn't correspond to any words. It can be used without altering the meaning of our sentences. First thing is to declare the maximum length of our tweets. Now, length of the tweet after encoding will be the number of words rather than it being the number of characters as it was before tokenizing. \n",
        "\n",
        "pad_sequences is the method used for padding. It takes the corpus, the value to pad with, the way we want to pad: 'post' indicates we want to pad at the end, maximum length will be the max length we just calculated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9qttbt7BMwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            value=0,\n",
        "                                                            padding=\"post\",\n",
        "                                                            maxlen=MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUE4SrotbEuz",
        "colab_type": "text"
      },
      "source": [
        "We need split our dataset into training and testing sets. Our data is actually ordered, the first half has negative sentiments while the latter half has positive sentiment. We will take 10% of the data as training set. To maintain the proportion of positive and negative sentiments in the test set to get an accurate resemblance of the accuracy, we need 1500 positive and 1500 negative tweets. (Totalling to 3000 tweets)\n",
        "\n",
        "We can generate 1500 random integers between 0 and 15000; 1500 random integers between 15001 and 30000. This can be done using random.randint() method of numpy. First arg: starting number, second arg: ending number, third arg: number of integers required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Ac7EXNNblp",
        "colab_type": "text"
      },
      "source": [
        "### Spliting into training/testing set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_H7zROsNbCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_neg_idx = np.random.randint(0, 15000, 1500)\n",
        "test_pos_idx = np.random.randint(15001, 30001, 1500)\n",
        "test_idx = np.concatenate((test_neg_idx, test_pos_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdHMgcMTe_9Y",
        "colab_type": "text"
      },
      "source": [
        "Using indices, we obtain the rows by accessing it like an array from both data_inputs and data_labels. Training inputs is obtained by deleting the test indices (randomly generated indices). \n",
        "\n",
        "Axis specifies if we want to remove along row or column. Since we need to remove row-wise, set axis to 0. Delete from both data_inputs and data_labels to generate data and targets in their respective variables. \n",
        "\n",
        "Axis is not required for labels since it's a vector and the only possible way to delete is row-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT7vDPUb06lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs = data_inputs[test_idx]\n",
        "test_labels = data_labels[test_idx]\n",
        "train_inputs = np.delete(data_inputs, test_idx, axis=0)\n",
        "train_labels = np.delete(data_labels, test_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWu6hLDG_UJZ",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC7_fgmIfvcA",
        "colab_type": "text"
      },
      "source": [
        "Architecture: Apply embedding layer. Then 3 different kinds of 1D convolution of size two (bigram), three (trigram) and four (fourgram). 1D convolution is used because the width is the model and filter is applied along one dimension. We apply a certain number of each of them. After applying the activation function for each filter, the output is a vector. We will take the max of each of those vectors via Max pooling and concatenate. Apply a linear function (Dense layer). Finally, our classification task is done. \n",
        "\n",
        "Our class is called DCNN stands for Deep Convolutional Neural Network that we inherit from keras.Model. Basically, we are building our own model. \n",
        "\n",
        "First is ithe ____init____ method which is the initialization function and has to be implemented for every model or layer in TensorFlow. The first parameter is self (to refer to the current object when instantiating a class). This is follwed by all the variables that we need to build our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD3nbD_M94Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128, #128 dimensions for embedding layer is default\n",
        "                 nb_filters=50, #number of filters = 50 (default value), number of times to apply convolution\n",
        "                 FFN_units=512, #number of units in the feed forward neural network = 512 (default value)\n",
        "                 nb_classes=2, #number of classes = 2 (positive or negative)\n",
        "                 dropout_rate=0.1, #default value. Dropout is a tool to turn off certain parameters and variables in order to avoid overfitting\n",
        "                 training=False, #boolean variable indicating if the model is in training phase. Mainly used to know if we need to apply dropout as dropout is only applied during training. \n",
        "                 name=\"dcnn\"): #name of our model\n",
        "        super(DCNN, self).__init__(name=name) #call the init function from the class we are inheriting from. Done by calling the super method giving the name of the class we are writing now and self. Give the name of our model to init method to initialize properly, \n",
        "        \n",
        "        #Defining layers\n",
        "        #1. Embedding layer with embedding dimensions and vocab size\n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2, #filter size\n",
        "                                    padding=\"valid\", #isn't very important which padding method is used because our stride (step size) is 1 (applying filter word by word). During last convolutions when the filter exceeds the length of the seuqence, valid method pads those spaces by zero. \n",
        "                                    activation=\"relu\") #ReLU (Rectified Linear Unit) is a standard activation function to introduce non-linearity into our model\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        \n",
        "        #1D Max Pooling since it's a 1D convolution \n",
        "        self.pool = layers.GlobalMaxPool1D() # no training variable so we can use the same layer for each pooling step\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\") #Dense layer \n",
        "        self.dropout = layers.Dropout(rate=dropout_rate) #Since there is a lot of variables and connections between them, this is a good place to apply Dropout to avoid overfitting\n",
        "        \n",
        "        #The last dense layer depends on how many classes we have. If there are 2 classes, we need a single number between 0 and 1 as the output.\n",
        "        #Below 0.5, belongs to class 0 (Negative sentiment). Above 0.5, belongs to class 1 (Positive sentiment)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1, #1 unit means a single number \n",
        "                                           activation=\"sigmoid\") #Sigmoid takes a number between -infinity and +infinity and returns a value between 0 and 1. This is the choice of activation in binary classification tasks\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\") #Softmax gives the number of values (equal to number of classes) between 0 and 1 whose sum is 1. It basically indicates the probability of belonging to each class\n",
        "    \n",
        "    #After defining the functions, we have to call them. Let's do this using a call function. This function gives outputs from inputs\n",
        "    def call(self, inputs, training): #self and inputs are obviously needed. training to indicate whether to apply dropout or not \n",
        "        x = self.embedding(inputs) #applying embedding\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters), axis = -1 indicates last axis where all the pooling values are present\n",
        "        merged = self.dense_1(merged) #First dense layer (starting feedforward process)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged) #outputs\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92XbAZ9E1AMS",
        "colab_type": "text"
      },
      "source": [
        "# Stage 4: Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8cfYwHME-m0",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXwGD-pqFG4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model parameters (Global variables)\n",
        "#Rather than passing the values as arguments, it's better to pass the variables containing those values. In this way, we can change all the values easily to modify our model\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "EMB_DIM = 200\n",
        "\n",
        "NB_FILTERS = 100\n",
        "\n",
        "FFN_UNITS = 256\n",
        "\n",
        "NB_CLASSES = len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NB_EPOCHS = 5\n",
        "#You can play around with these parameters (hyperparameter tuning) to acheive the highest accuracy."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nddzr1kA7UHC",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETcf5Wl4Q-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating an instance of the model and pass all the required parameters as defined before\n",
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCuNhMNk4n_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model compilation depending on the number of classes\n",
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\", #standard loss in binary classification\n",
        "                 optimizer=\"adam\", #standard\n",
        "                 metrics=[\"accuracy\"]) #metrics to track during training\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVyKbyB36IB2",
        "colab_type": "text"
      },
      "source": [
        "Let's create a checkpoint before training our model. This is a way to store our model once it's trained so that we need not train from scratch when we want to use it later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1X7h6Bx5Upc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the path\n",
        "checkpoint_path = \"./drive/My Drive/NLP/ckpt/\" \n",
        "\n",
        "#Creating checkpoint object\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "#Creating checkpoint manager\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5) #max_to_keep is maximum number of checkpoints we want to keep\n",
        "\n",
        "#Checking if there is already a checkpoint in the checkpoint path. If so, we will restore it and print a message saying the same.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eL9EMsa6Igy",
        "colab_type": "code",
        "outputId": "1454d2a1-9099-450a-8710-62dc42259dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#Fitting the model\n",
        "Dcnn.fit(train_inputs,\n",
        "         train_labels,\n",
        "         batch_size=BATCH_SIZE,\n",
        "         epochs=NB_EPOCHS)\n",
        "#Saving the checkpoint after training\n",
        "ckpt_manager.save()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "849/849 [==============================] - 56s 66ms/step - loss: 0.5422 - accuracy: 0.7159\n",
            "Epoch 2/5\n",
            "849/849 [==============================] - 57s 67ms/step - loss: 0.4216 - accuracy: 0.8064\n",
            "Epoch 3/5\n",
            "849/849 [==============================] - 56s 66ms/step - loss: 0.2888 - accuracy: 0.8797\n",
            "Epoch 4/5\n",
            "849/849 [==============================] - 62s 73ms/step - loss: 0.1313 - accuracy: 0.9518\n",
            "Epoch 5/5\n",
            "849/849 [==============================] - 56s 65ms/step - loss: 0.0799 - accuracy: 0.9704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./drive/My Drive/NLP/ckpt/ckpt-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Gn6JhJKXDK",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjyrNWEk7vVC",
        "colab_type": "text"
      },
      "source": [
        "Let's see how our model performs on new or unknown data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt2dRZWhKHbT",
        "colab_type": "code",
        "outputId": "8e7644e6-f600-4b9c-ce68-c588b22ad172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)\n",
        "#Ouputs [loss, accuracy]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 1s 16ms/step - loss: 0.9547 - accuracy: 0.7293\n",
            "[0.9547498822212219, 0.7293333411216736]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieambifb8bxg",
        "colab_type": "code",
        "outputId": "eb3f4dcb-f6f9-4708-95cd-080711d0dcc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgNeSn1t85qm",
        "colab_type": "text"
      },
      "source": [
        "For testing our own sentences, we have to encode the sentence using Tokenizer and convert into a numpy array. Set the training variable to False as we aren't training and dropout isn't required. The output will be a tensor which is hard to read. So, let's convert it into numpy format.\n",
        "\n",
        "Output value very close to zero indicates negative sentiment while close to 1 indicate positive sentiment. You can also choose a threshold for classification. A standard one is 0.5 (Above 0.5, positive sentiment and below 0.5, negative sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te0SJTWe2EVb",
        "colab_type": "code",
        "outputId": "ecb846b8-5a02-4a6a-f2a8-13597f72b556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"He is the best\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9838066]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fluAieSNsKbo",
        "colab_type": "code",
        "outputId": "103e6a87-8925-4445-d5ca-c50b1f838c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"Doesn't make sense\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00184285]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_uw8yj_sOlL",
        "colab_type": "code",
        "outputId": "0433863c-80a6-4133-ab25-c60b967f2a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"He sucks at playing\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01019072]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UJIzfCosW_t",
        "colab_type": "code",
        "outputId": "3ad13ea8-0889-43b1-d4a7-721e33b998fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"Why does he look ugly\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.06169504]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIqJqoRZsawx",
        "colab_type": "code",
        "outputId": "6aec4d20-7fd4-4ba0-e3db-438a7d055e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"He is a great guy\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999815]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlWi_jca9d9w",
        "colab_type": "code",
        "outputId": "a5137e9d-4582-4d10-e2e8-cb30c50d7123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"You are so funny\")]), training=False).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9976598]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ha7mYKg9_iW",
        "colab_type": "text"
      },
      "source": [
        "We had good results on the test dataset and it seems to work pretty well on our sentences. Of course, the data used is far from perfect. They are just few tweets. You have to get your dataset depending on the task at hand. This was a good dataset to show that our model works really well. They may not contain all the possible words. This was a good dataset to show that our model works pretty well. You can also try with your sentences or even your own dataset. In this project we started from scratch, learnt theory and implemented a sentiment analyzer. Hope you like it!"
      ]
    }
  ]
}